# -*- coding: utf-8 -*-
"""IPL_score_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-tb0sUTW60Kwl2QS2qiufH0Fe0c1t9DT
"""

# Importing essential libraries
import pandas as pd
import numpy as np

# Loading the dataset
df = pd.read_csv('ipl.csv')

df.columns

df.shape

df.dtypes

df.head()

#Data Cleaning
df.columns

# Removing unwanted columns
columns_to_remove = ['mid', 'venue', 'batsman', 'bowler', 'striker', 'non-striker']

print('Before removing unwanted columns: {}'.format(df.shape))
df.drop(labels=columns_to_remove, axis=1, inplace=True)
print('After removing unwanted columns: {}'.format(df.shape))

df.columns

df.head()

df.index

df.isnull().sum()

df['bat_team'].unique()

consistent_teams = ['Kolkata Knight Riders', 'Chennai Super Kings', 'Rajasthan Royals',
                    'Mumbai Indians', 'Kings XI Punjab', 'Royal Challengers Bangalore',
                    'Delhi Daredevils', 'Sunrisers Hyderabad']

# Keeping only consistent teams
print('Before removing inconsistent teams: {}'.format(df.shape))
df = df[(df['bat_team'].isin(consistent_teams)) & (df['bowl_team'].isin(consistent_teams))]
print('After removing inconsistent teams: {}'.format(df.shape))

df['bat_team'].unique()

# Removing the first 5 overs data in every match
print('Before removing first 5 overs data: {}'.format(df.shape))
df = df[df['overs']>=5.0]
print('After removing first 5 overs data: {}'.format(df.shape))

# Converting the column 'date' from string into datetime object
from datetime import datetime
print("Before converting 'date' column from string to datetime object: {}".format(type(df.iloc[0,0])))
df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))
print("After converting 'date' column from string to datetime object: {}".format(type(df.iloc[0,0])))

# Selecting only numeric columns
numeric_df = df.select_dtypes(include=['number'])

# Correlation matrix
corr_matrix = numeric_df.corr()
print(corr_matrix)

# Visualize as a heatmap
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(13, 10))
sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Covariance matrix
cov_matrix = numeric_df.cov()
print(cov_matrix)

# Visualize as a heatmap
plt.figure(figsize=(13, 10))
sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Covariance Matrix')
plt.show()

sns.countplot(data=df,x='runs')
plt.show()

sns.countplot(data=df,x='wickets')
plt.show()

sns.countplot(data=df,x='total')
plt.show()

#Data Preprocessing
""" Handling categorical features
Splitting dataset into train and test set on the basis of date"""

# Converting categorical features using OneHotEncoding method
encoded_df = pd.get_dummies(data=df, columns=['bat_team', 'bowl_team'])
encoded_df.columns

encoded_df.head()

# Rearranging the columns
encoded_df = encoded_df[['date', 'bat_team_Chennai Super Kings', 'bat_team_Delhi Daredevils', 'bat_team_Kings XI Punjab',
              'bat_team_Kolkata Knight Riders', 'bat_team_Mumbai Indians', 'bat_team_Rajasthan Royals',
              'bat_team_Royal Challengers Bangalore', 'bat_team_Sunrisers Hyderabad',
              'bowl_team_Chennai Super Kings', 'bowl_team_Delhi Daredevils', 'bowl_team_Kings XI Punjab',
              'bowl_team_Kolkata Knight Riders', 'bowl_team_Mumbai Indians', 'bowl_team_Rajasthan Royals',
              'bowl_team_Royal Challengers Bangalore', 'bowl_team_Sunrisers Hyderabad',
              'overs', 'runs', 'wickets', 'runs_last_5', 'wickets_last_5', 'total']]

# Splitting the data into train and test set
X_train = encoded_df.drop(labels='total', axis=1)[encoded_df['date'].dt.year <= 2016]
X_test = encoded_df.drop(labels='total', axis=1)[encoded_df['date'].dt.year >= 2017]

y_train = encoded_df[encoded_df['date'].dt.year <= 2016]['total'].values
y_test = encoded_df[encoded_df['date'].dt.year >= 2017]['total'].values

# Removing the 'date' column
X_train.drop(labels='date', axis=True, inplace=True)
X_test.drop(labels='date', axis=True, inplace=True)

print("Training set: {} and Test set: {}".format(X_train.shape, X_test.shape))

"""
Model Building
I will experiment with 5 different algorithms, they are as follows:
• Linear Regression
• Decision Tree Regression
• Random Forest Regression

----- Boosting Algorithm -----
• Adaptive Boosting (AdaBoost) Algorithm"""

# Linear Regression Model
from sklearn.linear_model import LinearRegression
linear_regressor = LinearRegression()
linear_regressor.fit(X_train,y_train)

# Predicting results
y_pred_lr = linear_regressor.predict(X_test)

# Linear Regression - Model Evaluation
from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse, accuracy_score
print("---- Linear Regression - Model Evaluation ----")
print("Mean Absolute Error (MAE): {}".format(mae(y_test, y_pred_lr)))
print("Mean Squared Error (MSE): {}".format(mse(y_test, y_pred_lr)))
print("Root Mean Squared Error (RMSE): {}".format(np.sqrt(mse(y_test, y_pred_lr))))

import matplotlib.pyplot as plt

# Predicted values
y_pred_lr = linear_regressor.predict(X_test)

# Scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_lr, alpha=0.7, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Prediction vs Actual Values')
plt.legend()
plt.grid()
plt.show()

from sklearn.metrics import r2_score

# Calculate the R^2 score
r2 = r2_score(y_test, y_pred_lr)
print("R^2 Score:", r2)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_pred_lr contains your predicted values
sns.distplot(y_pred_lr, hist=True, kde=True,
             bins=30, color='blue',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2})

plt.title('Distribution of Predicted Values')
plt.xlabel('Predicted Values')
plt.ylabel('Frequency')
plt.show()

# Decision Tree Regression Model
from sklearn.tree import DecisionTreeRegressor
decision_regressor = DecisionTreeRegressor()
decision_regressor.fit(X_train,y_train)

# Predicting results
y_pred_dt = decision_regressor.predict(X_test)

# Decision Tree Regression - Model Evaluation
print("---- Decision Tree Regression - Model Evaluation ----")
print("Mean Absolute Error (MAE): {}".format(mae(y_test, y_pred_dt)))
print("Mean Squared Error (MSE): {}".format(mse(y_test, y_pred_dt)))
print("Root Mean Squared Error (RMSE): {}".format(np.sqrt(mse(y_test, y_pred_dt))))

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_pred_lr contains your predicted values
sns.distplot(y_pred_dt, hist=True, kde=True,
             bins=30, color='blue',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2})

plt.title('Distribution of Predicted Values')
plt.xlabel('Predicted Values')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming `X_train` is your training data with feature names
feature_importances = decision_regressor.feature_importances_
features = X_train.columns  # Replace with your feature column names

# Plot feature importances
plt.figure(figsize=(10, 6))
indices = np.argsort(feature_importances)[::-1]  # Sort in descending order
plt.bar(range(len(feature_importances)), feature_importances[indices], color='skyblue', align='center')
plt.xticks(range(len(feature_importances)), [features[i] for i in indices], rotation=90)
plt.title("Feature Importances in Decision Tree Regressor")
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.tight_layout()
plt.grid()
plt.show()

# Random Forest Regression Model
from sklearn.ensemble import RandomForestRegressor
random_regressor = RandomForestRegressor()
random_regressor.fit(X_train,y_train)

# Predicting results
y_pred_rf = random_regressor.predict(X_test)

r2 = r2_score(y_test, y_pred_rf)
print("R^2 Score:", r2)

# Random Forest Regression - Model Evaluation
print("---- Random Forest Regression - Model Evaluation ----")
print("Mean Absolute Error (MAE): {}".format(mae(y_test, y_pred_rf)))
print("Mean Squared Error (MSE): {}".format(mse(y_test, y_pred_rf)))
print("Root Mean Squared Error (RMSE): {}".format(np.sqrt(mse(y_test, y_pred_rf))))

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_pred_lr contains your predicted values
sns.distplot(y_pred_rf, hist=True, kde=True,
             bins=30, color='blue',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2})

plt.title('Distribution of Predicted Values')
plt.xlabel('Predicted Values')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming `X_train` is your training data with feature names
feature_importances = random_regressor.feature_importances_
features = X_train.columns  # Replace with your feature column names

# Plot feature importances
plt.figure(figsize=(10, 6))
indices = np.argsort(feature_importances)[::-1]  # Sort in descending order
plt.bar(range(len(feature_importances)), feature_importances[indices], color='skyblue', align='center')
plt.xticks(range(len(feature_importances)), [features[i] for i in indices], rotation=90)
plt.title("Feature Importances in Random Forest Regressor")
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.tight_layout()
plt.grid()
plt.show()

# Scatter plot for predictions vs actual values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_rf, color='green', alpha=0.7, label='Predicted vs Actual')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Fit')
plt.title("Random Forest Regressor: Predictions vs Actual Values")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.legend()
plt.grid()
plt.show()

#AdaBoost Algorithm


from sklearn.ensemble import AdaBoostRegressor

adb_regressor = AdaBoostRegressor(estimator=linear_regressor, n_estimators=100)
adb_regressor.fit(X_train, y_train)

# Predicting results
y_pred_adb = adb_regressor.predict(X_test)

# AdaBoost Regression - Model Evaluation
print("---- AdaBoost Regression - Model Evaluation ----")
print("Mean Absolute Error (MAE): {}".format(mae(y_test, y_pred_adb)))
print("Mean Squared Error (MSE): {}".format(mse(y_test, y_pred_adb)))
print("Root Mean Squared Error (RMSE): {}".format(np.sqrt(mse(y_test, y_pred_adb))))

import matplotlib.pyplot as plt
import numpy as np


results = {
    'Algorithm': ['Linear Regression', 'Decision Tree', 'Random Forest', 'AdaBoost'],
    'MAE': [12.11861754619329, 17.159827213822894, 13.712752399808016, 12.209819020585646],
    'RMSE': [15.843229566732099, 23.031718198965187, 18.222892855283124, 15.808364111053095],
    'R²': [251.00792310417415, 530.4600431965442, 332.07382401512876, 249.9043758676315]
}

# Convert to numpy arrays for easier plotting
algorithms = np.array(results['Algorithm'])
mae = np.array(results['MAE'])
rmse = np.array(results['RMSE'])
r2 = np.array(results['R²'])

# Bar plot for MAE and RMSE
x = np.arange(len(algorithms))  # Label locations
width = 0.35  # Width of the bars

fig, ax1 = plt.subplots(figsize=(10, 6))

# Plotting MAE and RMSE
ax1.bar(x - width/2, mae, width, label='MAE', color='skyblue')
ax1.bar(x + width/2, rmse, width, label='RMSE', color='salmon')

# Adding labels
ax1.set_xlabel('Algorithm')
ax1.set_ylabel('Error')
ax1.set_title('Comparison of Algorithms (Error Metrics)')
ax1.set_xticks(x)
ax1.set_xticklabels(algorithms)
ax1.legend()

# Line plot for R² score on secondary axis
ax2 = ax1.twinx()  # Secondary y-axis
ax2.plot(x, r2, label='R² Score', color='green', marker='o', linewidth=2)
ax2.set_ylabel('R² Score')
ax2.legend(loc='upper left')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Train the AdaBoost model
adb_regressor = AdaBoostRegressor(n_estimators=100)
adb_regressor.fit(X_train, y_train)

# Predict the results
y_pred_adb = adb_regressor.predict(X_test)

# Scatter plot to compare actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_adb, alpha=0.7, color='purple')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('AdaBoost Regressor: Predictions vs Actual Values')
plt.legend()
plt.grid(True)
plt.show()

# Print performance metrics
print("R^2 Score:", r2_score(y_test, y_pred_adb))

plt.figure(figsize=(10, 6))
plt.hist(df['runs_last_5'], bins=30, color='g', alpha=0.7)

plt.xlabel('X-axis Label')
plt.ylabel('Frequency')
plt.title('Histogram of Continuous Data')
plt.grid(True)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_pred_lr contains your predicted values
sns.distplot(y_pred_adb, hist=True, kde=True,
             bins=30, color='blue',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2})

plt.title('Distribution of Predicted Values')
plt.xlabel('Predicted Values')
plt.ylabel('Frequency')
plt.show()

"""
Predictions
• Model trained on the data from IPL Seasons 1 to 9 ie: (2008 to 2016)
• Model tested on data from IPL Season 10 ie: (2017)
• Model predicts on data from IPL Seasons 11 to 12 ie: (2018 to 2019)
"""

def predict_score(batting_team='Chennai Super Kings', bowling_team='Mumbai Indians', overs=5.1, runs=50, wickets=0, runs_in_prev_5=50, wickets_in_prev_5=0):
  temp_array = list()

  # Batting Team
  if batting_team == 'Chennai Super Kings':
    temp_array = temp_array + [1,0,0,0,0,0,0,0]
  elif batting_team == 'Delhi Daredevils':
    temp_array = temp_array + [0,1,0,0,0,0,0,0]
  elif batting_team == 'Kings XI Punjab':
    temp_array = temp_array + [0,0,1,0,0,0,0,0]
  elif batting_team == 'Kolkata Knight Riders':
    temp_array = temp_array + [0,0,0,1,0,0,0,0]
  elif batting_team == 'Mumbai Indians':
    temp_array = temp_array + [0,0,0,0,1,0,0,0]
  elif batting_team == 'Rajasthan Royals':
    temp_array = temp_array + [0,0,0,0,0,1,0,0]
  elif batting_team == 'Royal Challengers Bangalore':
    temp_array = temp_array + [0,0,0,0,0,0,1,0]
  elif batting_team == 'Sunrisers Hyderabad':
    temp_array = temp_array + [0,0,0,0,0,0,0,1]

  # Bowling Team
  if bowling_team == 'Chennai Super Kings':
    temp_array = temp_array + [1,0,0,0,0,0,0,0]
  elif bowling_team == 'Delhi Daredevils':
    temp_array = temp_array + [0,1,0,0,0,0,0,0]
  elif bowling_team == 'Kings XI Punjab':
    temp_array = temp_array + [0,0,1,0,0,0,0,0]
  elif bowling_team == 'Kolkata Knight Riders':
    temp_array = temp_array + [0,0,0,1,0,0,0,0]
  elif bowling_team == 'Mumbai Indians':
    temp_array = temp_array + [0,0,0,0,1,0,0,0]
  elif bowling_team == 'Rajasthan Royals':
    temp_array = temp_array + [0,0,0,0,0,1,0,0]
  elif bowling_team == 'Royal Challengers Bangalore':
    temp_array = temp_array + [0,0,0,0,0,0,1,0]
  elif bowling_team == 'Sunrisers Hyderabad':
    temp_array = temp_array + [0,0,0,0,0,0,0,1]

  # Overs, Runs, Wickets, Runs_in_prev_5, Wickets_in_prev_5
  temp_array = temp_array + [overs, runs, wickets, runs_in_prev_5, wickets_in_prev_5]

  # Converting into numpy array
  temp_array = np.array([temp_array])

  # Prediction
  return int(linear_regressor.predict(temp_array)[0])

"""
Prediction 1
• Date: 16th April 2018
• IPL : Season 11
• Match number: 13
• Teams: Kolkata Knight Riders vs. Delhi Daredevils
• First Innings final score: 200/9
"""

final_score = predict_score(batting_team='Kolkata Knight Riders', bowling_team='Delhi Daredevils', overs=9.2, runs=79, wickets=2, runs_in_prev_5=60, wickets_in_prev_5=1)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

"""
Prediction 2
• Date: 17th May 2018
• IPL : Season 11
• Match number: 50
• Teams: Mumbai Indians vs. Kings XI Punjab
• First Innings final score: 186/8
"""
final_score = predict_score(batting_team='Mumbai Indians', bowling_team='Kings XI Punjab', overs=14.1, runs=136, wickets=4, runs_in_prev_5=50, wickets_in_prev_5=0)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

"""
Prediction 3
• Date: 7th May 2018
• IPL : Season 11
• Match number: 39
• Teams: Sunrisers Hyderabad vs. Royal Challengers Bangalore
• First Innings final score: 146/10
"""
final_score = predict_score(batting_team='Sunrisers Hyderabad', bowling_team='Royal Challengers Bangalore', overs=10.5, runs=67, wickets=3, runs_in_prev_5=29, wickets_in_prev_5=1)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

"""
Prediction 4
• Date: 30th March 2019
• IPL : Season 12
• Match number: 9
• Teams: Mumbai Indians vs. Kings XI Punjab
• First Innings final score: 176/7
"""
final_score = predict_score(batting_team='Mumbai Indians', bowling_team='Kings XI Punjab', overs=12.3, runs=113, wickets=2, runs_in_prev_5=55, wickets_in_prev_5=0)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

"""
Prediction 5
• Date: 11th April 2019
• IPL : Season 12
• Match number: 25
• Teams: Rajasthan Royals vs. Chennai Super Kings
• First Innings final score: 151/7
"""
final_score = predict_score(batting_team='Rajasthan Royals', bowling_team='Chennai Super Kings', overs=13.3, runs=92, wickets=5, runs_in_prev_5=27, wickets_in_prev_5=2)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

"""
Prediction 7
• Date: 10th May 2019
• IPL : Season 12
• Match number: 59 (Eliminator)
• Teams: Delhi Daredevils vs. Chennai Super Kings
• First Innings final score: 147/9
"""
final_score = predict_score(batting_team='Delhi Daredevils', bowling_team='Chennai Super Kings', overs=10.2, runs=68, wickets=3, runs_in_prev_5=29, wickets_in_prev_5=1)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))

final_score = predict_score(batting_team='Delhi Daredevils', bowling_team='Chennai Super Kings', overs=10.2, runs=68, wickets=3, runs_in_prev_5=29, wickets_in_prev_5=1)
print("The final predicted score (range): {} to {}".format(final_score-10, final_score+5))



